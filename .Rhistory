require(rstan)
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies = TRUE)
require(rstan)
install.packages("tranlateR")
install.packages("RSelenium")
install.packages("rvest")
install.packages("tidyverse")
install.packages("RSelenium")
install.packages("rvest")
require(RSelenium)
require(rvest)
require(tidyverse)
rD <- rsDriver(browser="firefox", port=4545L, verbose=F)
remDr <- rD[["client"]]
remDr$navigate("https://www.fcc.gov/media/engineering/dtvmaps")
zip <- "30308"
remDr$findElement(using = "id", value = "startpoint")$sendKeysToElement(list(zip))
remDr$findElements("id", "btnSub")[[1]]$clickElement()
remDr$findElements("id", "btnSub")[[1]]$clickElement()
remDr$findElements("id", "btnSub")[[1]]$clickElement()
exp(-10)
exp(5)
exp(1)
exp(0.5)
exp(0)
exp(0.0001)
exp(-1)
exp(-10)
exp(-10+1)
exp(-10-1)
log(0.000123)
log(0.1)
log(0.01)
log(0.001)
log(0.00001)
####
# This file illustrates dictionary methods and sentiment analysis on FOMC minutes
####
setwd("~/Documents/GitHub/intro-text-analysis-econ")
rm(list = ls())
require(stringr)
require(tidyverse)
require(tidytext)
require(ggplot2)
require(ggwordcloud)
std <- function(x){
x <- (x - mean(x, na.rm = T))/sd(x, na.rm = T)
return(x)
}
############### Examples of dictionaries ###############
affin <- get_sentiments("afinn")
loughran <- get_sentiments("loughran")
############### Import data ###############
# Minutes
fedminutes_df <- read.csv("data/fedminutes_all.csv", stringsAsFactors = FALSE)
# GDP growth
gdp_df <- read.csv("data/GDPC1.csv", stringsAsFactors = FALSE) %>%
rename(quarter = DATE, gdp = GDPC1) %>%
mutate(gdp_lag = lag(gdp),
growth = 100*(log(gdp) - log(gdp_lag)),
quarter = as.Date(quarter))
# Separate out words
fedminutes_words <- fedminutes_df %>%
mutate(paragraph = str_remove_all(paragraph, "[0-9]+")) %>%
select(date, paragraph) %>%
unnest_tokens(word, paragraph) %>%
group_by(date) %>% count(word, sort = F) %>% ungroup()
fedminutes_words
# Identify words from list
fedminutes_sentiment <- fedminutes_words %>%
inner_join(get_sentiments("loughran"))
# Look at the most relevant words in each category
fedminutes_total <- fedminutes_sentiment %>%
group_by(word, sentiment) %>%
summarise(n = sum(n)) %>%
filter(n > 25)
fedminutes_total
# Plot frequent words
ggplot(fedminutes_total, aes(label = word, size = n)) +
facet_wrap(.~sentiment) +
geom_text_wordcloud() +
theme_minimal()
# Aggregate to meeting level
fedminutes_agg <- fedminutes_sentiment %>%
pivot_wider(id_cols = c(date, word), names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(quarter = floor_date(as.Date(date), unit = "quarter")) %>%
group_by(quarter) %>%
summarise(across(where(is.numeric), sum)) %>%
mutate(polarity = (positive - negative)/(1+positive+negative),
quarter = as.Date(quarter)) %>%
inner_join(gdp_df)
ggplot(filter(fedminutes_agg,quarter >= "2000-01-01")) + theme_bw() +
geom_line(aes(x = quarter, y = growth, color = "GDP")) +
geom_line(aes(x = quarter, y = std(polarity), color = "loughran"))
fedminutes_agg <- fedminutes_sentiment %>%
pivot_wider(id_cols = c(date, word), names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(quarter = floor_date(as.Date(date), unit = "quarter")) %>%
group_by(quarter) %>%
summarise(across(where(is.numeric), sum)) %>%
mutate(polarity = (positive - negative)/(1+positive+negative),
quarter = as.Date(quarter)) %>%
inner_join(gdp_df)
require(lubridate)
fedminutes_agg <- fedminutes_sentiment %>%
pivot_wider(id_cols = c(date, word), names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(quarter = floor_date(as.Date(date), unit = "quarter")) %>%
group_by(quarter) %>%
summarise(across(where(is.numeric), sum)) %>%
mutate(polarity = (positive - negative)/(1+positive+negative),
quarter = as.Date(quarter)) %>%
inner_join(gdp_df)
# Captures the GFC well, but not the Covid shock - not a conventional economic crisis!
ggplot(filter(fedminutes_agg,quarter >= "2000-01-01")) + theme_bw() +
geom_line(aes(x = quarter, y = growth, color = "GDP")) +
geom_line(aes(x = quarter, y = std(polarity), color = "loughran"))
fedminutes_qly <- fedminutes_df %>%
mutate(quarter = floor_date(as.Date(date), unit = "quarter")) %>%
select(quarter, paragraph) %>%
group_by(quarter) %>%
summarise(text = paste(paragraph, collapse = "\n"))
require(vader)
text <- "Julian is a nice person"
get_vader(text)
text <- "Julian is a very nice person"
get_vader(text)
text <- "Julian is not a very nice person"
get_vader(text)
####
# This file illustrates supervised topic model on FOMC minutes
####
setwd("~/Documents/GitHub/intro-text-analysis-econ")
rm(list = ls())
require(ggplot2)
require(forcats)
require(glmnet)
require(lda)
############### Import data ###############
# GDP growth
gdp_df <- read.csv("data/GDPC1.csv", stringsAsFactors = FALSE) %>%
rename(quarter = DATE, gdp = GDPC1) %>%
mutate(gdp_lag = lag(gdp),
growth = 100*(log(gdp) - log(gdp_lag)),
quarter = as.Date(quarter))
# Minutes
fedminutes_df <- read.csv("data/fedminutes_clean.csv", stringsAsFactors = FALSE) %>%
mutate(quarter = floor_date(as.Date(date), unit = "quarter")) %>%
left_join(gdp_df)
fedminutes_qly <- fedminutes_df %>%
group_by(quarter) %>%
summarize(paragraph_clean = paste(paragraph_clean, collapse = " ")) %>%
left_join(gdp_df)
growth_slda <- readRDS(file = "data/mins_slda_10.rds")
Topics <- apply(top.topic.words(growth_slda$topics, 10, by.score=TRUE),
2, paste, collapse=" ")
coefs <- data.frame(coef(summary(growth_slda$model)))
coefs <- cbind(coefs, Topics=factor(Topics, Topics[order(coefs$Estimate)]))
coefs <- coefs[order(coefs$Estimate),]
# Plot the coefficients
ggplot(coefs,aes(x = Estimate, y = Topics)) + theme_bw() +
geom_point() +
geom_errorbar(width=0.5, aes(xmin= Estimate - Std..Error,
xmax= Estimate + Std..Error))
####
# This file illustrates topic models on FOMC minutes
####
setwd("~/Documents/GitHub/intro-text-analysis-econ")
rm(list = ls())
require(ggplot2)
require(forcats)
require(doc2vec)
require(tokenizers.bpe)
require(udpipe)
############### Import data ###############
# GDP growth
gdp_df <- read.csv("data/GDPC1.csv", stringsAsFactors = FALSE) %>%
rename(quarter = DATE, gdp = GDPC1) %>%
mutate(gdp_lag = lag(gdp),
growth = 100*(log(gdp) - log(gdp_lag)),
quarter = as.Date(quarter))
# Minutes
fedminutes_df <- read.csv("data/fedminutes_all.csv", stringsAsFactors = FALSE) %>%
mutate(quarter = floor_date(as.Date(date), unit = "quarter")) %>%
inner_join(gdp_df) %>%
filter(quarter > "2000-01-01")
############### Embedding ###############
embedding_df <- fedminutes_df %>%
rename(doc_id = unique_id,
text = paragraph) %>%
mutate(text = str_squish(gsub("[^[:alpha:]]", " ", tolower(text))))
model <- paragraph2vec(x = embedding_df, type = "PV-DBOW", dim = 100, iter = 20,
min_count = 5, lr = 0.05, threads = 4)
View(fedminutes_df)
embedding_df <- fedminutes_df %>%
rename(doc_id = unique_id,
text = paragraph) %>%
mutate(text = str_squish(gsub("[^[:alpha:]]", " ", tolower(text))))
embedding_df$text[1]
predict(model, newdata = c("increase", "inflation", "growth"),
type = "nearest", which = "word2word", top_n = 5)
word_embeddings <- as.matrix(model, which = "words")
dim(word_embeddings)
rownames(word_embeddings) <- summary(model, which = "words")
plot(word_embeddings["decline",], type = "l")
lines(word_embeddings["increase",], col = "red")
lines(word_embeddings["canada",], col = "blue")
cor(word_embeddings["decline",], word_embeddings["increase",])
cor(word_embeddings["decline",], word_embeddings["canada",])
doc_embeddings <- as.matrix(model, which = "docs")
dim(doc_embeddings)
rownames(doc_embeddings)
doc_pca <- data.frame(prcomp(doc_embeddings)$x)
doc_pca$doc_id <- rownames(doc_pca)
embedding_df <- merge(embedding_df, doc_pca, by = "doc_id")
embedding_df$Period <- "pre-2008"
embedding_df$Period[which(embedding_df$quarter<="2008-01-01")] <- "2008-present"
embedding_df$Recession <- as.numeric(embedding_df$growth<=0)
embedding_df$Period <- "pre-2008"
embedding_df$Period[which(embedding_df$quarter>="2008-01-01")] <- "2008-present"
embedding_df <- merge(embedding_df, doc_pca, by = "doc_id")
embedding_df$Period <- "pre-2008"
embedding_df$Period[which(embedding_df$quarter>="2008-01-01")] <- "2008-present"
embedding_df$Recession <- as.numeric(embedding_df$growth<=0)
ggplot(embedding_df) + theme_bw() +
geom_point(aes(x = PC1, y = PC2, color = Period), alpha = 0.2)
names(embedding_df)
ggplot(embedding_df) + theme_bw() +
geom_point(aes(x = PC1.x, y = PC2.x, color = Period), alpha = 0.2)
summary(lm(PC4~ Recession, embedding_df))
summary(lm(PC4.x~ Recession, embedding_df))
require(tm)
fedminutes_df$paragraph[2000]
fedminutes_df$paragraph_clean <- tolower(fedminutes_df$paragraph)
fedminutes_df$paragraph_clean[2000]
fedminutes_df$paragraph_clean <- gsub("[^[:alpha:]]", " ", fedminutes_df$paragraph_clean)
fedminutes_df$paragraph_clean[2000]
fedminutes_df$paragraph_clean <- sapply(tm_map(Corpus(VectorSource(unlist(fedminutes_df$paragraph_clean))),
removeWords, stopwords("english")), as.character)
fedminutes_df$paragraph_clean[2000]
fedminutes_df$paragraph_clean <- sapply(tm_map(Corpus(VectorSource(unlist(fedminutes_df$paragraph_clean))),
stemDocument), as.character)
fedminutes_df$paragraph_clean[2000]
fedminutes_df$paragraph_clean <- str_squish(fedminutes_df$paragraph_clean)
fedminutes_df$paragraph_clean[2000]
fedminutes_df <- filter(fedminutes_df, nchar(paragraph_clean) > 2)
require(topicmodels)
corpus <- Corpus(VectorSource(unlist(fedminutes_df$paragraph_clean)))
mins_dtm <- DocumentTermMatrix(corpus, control = list(minsWordLength = 3))
fedminutes_df$wordcount <- rowSums(as.matrix(mins_dtm))
ggplot(fedminutes_df, aes(x = wordcount)) + geom_histogram()
# Look at term frequencies
mins_vocab <- mins_dtm$dimnames$Terms
vocab_df <- data.frame(term = mins_vocab, tf = colSums(as.matrix(mins_dtm)))
# List of words to remove
remove_words <- vocab_df$term[which(vocab_df$tf < 5)]
# Remove words
fedminutes_df$paragraph_clean <- sapply(tm_map(Corpus(VectorSource(unlist(fedminutes_df$paragraph_clean))),
removeWords, remove_words), as.character)
fedminutes_df$paragraph_clean[2000]
corpus <- Corpus(VectorSource(unlist(fedminutes_df$paragraph_clean)))
mins_dtm <- DocumentTermMatrix(corpus, control = list(minsWordLength = 3))
mins_dtm$dimnames$Docs <- fedminutes_df$unique_id
mins_dtm
set.seed(1234)
lda_gibbs <- readRDS(file = "data/mins_lda_20.rds")
word_topic_dist <- tidy(lda_gibbs, matrix = "beta")
word_topic_dist
top_terms <- word_topic_dist %>%
group_by(topic) %>%
top_n(10,beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms
top_terms$term[duplicated(top_terms$term)] <- paste0(top_terms$term[duplicated(top_terms$term)], " ")
top_terms$term[duplicated(top_terms$term)] <- paste0(top_terms$term[duplicated(top_terms$term)], " ")
top_terms$term[duplicated(top_terms$term)] <- paste0(top_terms$term[duplicated(top_terms$term)], " ")
top_terms$term[duplicated(top_terms$term)] <- paste0(top_terms$term[duplicated(top_terms$term)], " ")
top_terms$term <- factor(top_terms$term, ordered = TRUE,
levels = rev(unique(top_terms$term)))
ggplot(top_terms) + theme_bw() +
facet_wrap(~ topic, scales = "free") +
geom_bar(aes(x = term, y = beta, fill = factor(topic)),
stat = "identity", show.legend = FALSE) +
coord_flip()
doc_topic_dist <- tidy(lda_gibbs, matrix = "gamma") %>%
rename(unique_id = document) %>%
inner_join(select(fedminutes_df, unique_id, quarter, growth)) %>%
filter(quarter >= "2000-01-01") %>%
group_by(topic, quarter) %>%
summarise(gamma = mean(gamma), growth = mean(growth))
ggplot(doc_topic_dist) + theme_bw() + facet_wrap(~ topic, scales = "free") +
geom_line(aes(x = quarter, y = gamma))
topic11_words <- filter(word_topic_dist, topic == 11) %>%
arrange(topic, -beta) %>%
top_n(30,beta) %>%
mutate(beta = beta/sum(beta))
ggplot(topic11_words, aes(label = term, size = beta)) +
geom_text_wordcloud() +
theme_minimal()
topic11_docs <- filter(doc_topic_dist, topic == 11)
ggplot(topic11_docs, aes(x = quarter, y = gamma)) +
geom_line() + theme_minimal()
set.seed(1234)
lda_gibbs <- LDA(mins_dtm, k = 20, method = "Gibbs",
control = list(verbose = 1000, burnin = 1000, thin = 10, iter = 5000))
predict(model, newdata = c("increase", "inflation", "growth"),
type = "nearest", which = "word2word", top_n = 5)
####
# This file illustrates supervised topic model on FOMC minutes
####
setwd("~/Documents/GitHub/intro-text-analysis-econ")
rm(list = ls())
require(ggplot2)
require(forcats)
require(glmnet)
require(lda)
# GDP growth
gdp_df <- read.csv("data/GDPC1.csv", stringsAsFactors = FALSE) %>%
rename(quarter = DATE, gdp = GDPC1) %>%
mutate(gdp_lag = lag(gdp),
growth = 100*(log(gdp) - log(gdp_lag)),
quarter = as.Date(quarter))
# Minutes
fedminutes_df <- read.csv("data/fedminutes_clean.csv", stringsAsFactors = FALSE) %>%
mutate(quarter = floor_date(as.Date(date), unit = "quarter")) %>%
left_join(gdp_df)
fedminutes_qly <- fedminutes_df %>%
group_by(quarter) %>%
summarize(paragraph_clean = paste(paragraph_clean, collapse = " ")) %>%
left_join(gdp_df)
corpus <- Corpus(VectorSource(unlist(fedminutes_qly$paragraph_clean)))
mins_dtm <- DocumentTermMatrix(corpus, control = list(minsWordLength = 3))
fedminutes_qly$nwords <-
#mins_dtm$dimnames$Docs <- fedminutes_qly$unique_id
mins_tf <- as.matrix(mins_dtm)[,which(colSums(as.matrix(mins_dtm)) > 4000 &
colSums(as.matrix(mins_dtm)) < 10000)]
mins_dtm
mins_tf <- as.matrix(mins_dtm)[,which(colSums(as.matrix(mins_dtm)) > 4000 &
colSums(as.matrix(mins_dtm)) < 10000)]
mins_tf <- sweep(mins_tf, 1, rowSums(mins_tf), "/")
mins_tf
lasso_model <- glmnet(mins_tf, fedminutes_qly$growth,
family="gaussian", intercept = T, alpha=1,
standardize = TRUE)
plot(lasso_model)
coeffs <- coef(lasso_model, s = 0.1)
coeffs_df <- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x)
coeffs_df
cv.fit <- cv.glmnet(mins_tf, fedminutes_qly$growth, standardise = TRUE)
plot(cv.fit)
cv.fit$lambda.min
coeffs <- coef(lasso_model, s = cv.fit$lambda.min)
coeffs_df <- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x)
coeffs_df
docs <- lexicalize(fedminutes_qly$paragraph_clean)
docs$vocab
docs$documents[[10]]
train_labels <- fedminutes_qly$growth
K <- 10
growth_slda <- readRDS(file = "data/mins_slda_10.rds")
Topics <- apply(top.topic.words(growth_slda$topics, 10, by.score=TRUE),
2, paste, collapse=" ")
coefs <- data.frame(coef(summary(growth_slda$model)))
coefs <- cbind(coefs, Topics=factor(Topics, Topics[order(coefs$Estimate)]))
coefs <- coefs[order(coefs$Estimate),]
ggplot(coefs,aes(x = Estimate, y = Topics)) + theme_bw() +
geom_point() +
geom_errorbar(width=0.5, aes(xmin= Estimate - Std..Error,
xmax= Estimate + Std..Error))
